# if given, will override the device setting in gym. 
env: 
  numEnvs: 4096
  envSpacing: 5
  episodeLength: 600
  episodeLengthShort: 300
  isFlagrun: False
  enableDebugVis: False
  
  pdControl: True
  powerScale: 1.0
  controlFrequencyInv: 2 # 30 Hz
  stateInit: "Random"
  hybridInitProb: 0.5
  numAMPObsSteps: 10

  localRootObs: True # amp disc obs
  localRootObsPolicy: False # policy obs
  rootHeightObs: False # amp disc obs
  rootHeightObsPolicy: False # policy obs
  keyBodies: ["right_hand", "left_hand", "right_foot", "left_foot"]
  contactBodies: ["right_foot", "left_foot"]
  terminationHeight: 0.15
  enableEarlyTermination: True

  enableSelfCollisionDetection: True
  enableTrackInitState: False

  mode: "train"
  
  skill: [
    "loco", 
    "loco_sit", "sit",
    "loco_carry", "omomo", "pickUp", "carryWith", "putDown", 
    "loco_climb", "climb", "climbNoRSI"
  ]
  skillDiscProb: [
    0.1,
    0.1, 0.2,
    0.1, 0.1, 0.05, 0.0, 0.05,
    0.1, 0.1, 0.1,
  ]
  commonSkill: ""
  enableTaskSpecificDisc: True

  # multi tasks
  enableTaskObs: True
  enableTaskMaskObs: True
  enableApplyMaskOnTaskObs: True

  task: ["traj", "sit", "carry", "climb"]
  taskInitProb: [0.1, 0.3, 0.3, 0.3]

  # IET: Interaction Early Termination
  enableIET: True
  maxIETSteps: 60
  successThreshold: 0.2
 
  traj:
    numTrajSamples: 10
    trajSampleTimestep: 0.5 # 10 Hz
    speedMin: 0.5
    speedMax: 1.5
    accelMax: 2.0
    sharpTurnProb: 0.02
    sharpTurnAngle: 1.57
    failDist: 4.0
    
    skill: ["loco"]
    skillInitProb: [1.0]

    eval:
      # override env settings
      ## if the humanoid's root joint is close enough to the traj's final tar pos, 
      ## the episode will be treated as successful.
      successThreshold: 0.3
      skill: ["loco"]
      skillInitProb: [1.0]
  
  sit:
    # object
    objDatasetDir: "dataset_sit/objects"
    objCategories: 
      - "StraightChair_Normal"

    sit_vel_penalty: True
    sit_vel_pen_coeff: 0.5
    sit_vel_pen_threshold: 1.3
    sit_ang_vel_pen_coeff: 0.5
    sit_ang_vel_pen_threshold: 3.0

    # IET: Interaction Early Termination
    enableIET: True

    skill: ["loco_sit", "sit"]
    skillInitProb: [0.5, 0.5]

    eval:
      # override env settings
      skill: ["loco_sit", "sit"]
      skillInitProb: [1.0, 0.0]

  climb:
    # object
    objDatasetDir: "dataset_amass_climb/objects"
    objCategories: 
      - "Box"
      - "Cabinet"
      - "Table_Square"
    
    climb_vel_penalty: True
    climb_vel_pen_coeff: 1.0
    climb_vel_pen_threshold: 1.3

    # IET: Interaction Early Termination
    enableIET: True
    
    skill: ["loco_climb", "climb", "climbNoRSI"]
    skillInitProb: [0.5, 0.5, 0.0]

    eval:
      # override env settings
      skill: ["loco_climb", "climb", "climbNoRSI"]
      skillInitProb: [1.0, 0.0, 0.0]

  carry:
    onlyVelReward: True
    onlyHeightHandHeldReward: False

    box_vel_penalty: True # to avoid stiff grasping
    box_vel_pen_coeff: 1.0
    box_vel_pen_threshold: 2.5

    box:
      build:
        baseSize: [0.4, 0.4, 0.4] # unit: m
        randomSize: True
        randomModeEqualProportion: True
        scaleRangeX: [0.5, 1.5] # default range if randomModeEqualProportion: True
        scaleRangeY: [0.5, 1.5]
        scaleRangeZ: [0.5, 1.5]
        scaleSampleInterval: 0.125 # 0.05m

        testSizes:
        - [0.22, 0.22, 0.22]
        - [0.27, 0.27, 0.27]
        - [0.32, 0.32, 0.32]
        - [0.37, 0.37, 0.37]
        - [0.42, 0.42, 0.42]
        - [0.47, 0.47, 0.47]
        - [0.52, 0.52, 0.52]
        - [0.57, 0.57, 0.57]
        - [0.30, 0.30, 0.40]

      reset:
        randomRot: True
        randomHeight: True
        randomHeightProb: 0.5
        maxTopSurfaceHeight: 1.2
        minBottomSurfaceHeight: 0.0

    skill: ["loco_carry", "pickUp", "carryWith", "putDown", "omomo"]
    skillInitProb: [0.5, 0.1, 0.3, 0.1, 0.0]

    eval:
      # override env settings
      skill: ["loco_carry", "pickUp", "carryWith", "putDown", "omomo"]
      skillInitProb: [1.0, 0.0, 0.0, 0.0, 0.0]

  # 顺序训练相关配置
  enableSequentialTraining: True  # 是否启用顺序训练
  taskSequence: [0, 1, 2, 3]  # 任务执行顺序
  taskEpochs: [0, 6000, 0, 0]  # 每个任务的训练轮数
    

  # power reward
  power_reward: True
  power_coefficient: 0.0005

  asset:
    assetRoot: "tokenhsi/data/assets"
    assetFileName: "mjcf/phys_humanoid_v3.xml"

  plane:
    staticFriction: 1.0
    dynamicFriction: 1.0
    restitution: 0.0

sim:
  substeps: 2
  physx:
    num_threads: 4
    solver_type: 1  # 0: pgs, 1: tgs
    num_position_iterations: 4
    num_velocity_iterations: 0
    contact_offset: 0.02
    rest_offset: 0.0
    bounce_threshold_velocity: 0.2
    max_depenetration_velocity: 10.0
    default_buffer_size_multiplier: 10.0

  flex:
    num_inner_iterations: 10
    warm_start: 0.25
